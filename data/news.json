{
  "lastUpdated": "2026-02-18T03:34:06.548Z",
  "items": [
    {
      "title": "India Fuels Its AI Mission With NVIDIA",
      "link": "https://blogs.nvidia.com/blog/india-ai-mission-infrastructure-models/",
      "description": "From AI infrastructure leaders to frontier model developers, India is teaming with NVIDIA to drive AI transformation across the nation.",
      "content": "From AI infrastructure leaders to frontier model developers, India is teaming with NVIDIA to drive AI transformation across the nation.",
      "author": "Jay Puri",
      "source": "Nvidia Blog",
      "sourceType": "news",
      "pubDate": "Wed, 18 Feb 2026 00:30:49 +0000",
      "popularity": 0,
      "category": "inference-other"
    },
    {
      "title": "India’s Global Systems Integrators Build Next Wave of Enterprise Agents With NVIDIA AI, Transforming Back Office and Customer Support",
      "link": "https://blogs.nvidia.com/blog/india-enterprise-ai-agents/",
      "description": "Agentic AI is reshaping India’s tech industry, delivering leaps in services worldwide. Tapping into NVIDIA AI Enterprise software and NVIDIA Nemotron models, India’s technology leaders are accelerating productivity and efficiency across industries — from call centers to telecommunications and healthcare. Infosys, Persistent, Tech Mahindra and Wipro are leading the way for business transformation, improving back-office\t\n\t\tRead Article",
      "content": "Agentic AI is reshaping India’s tech industry, delivering leaps in services worldwide. Tapping into NVIDIA AI Enterprise software and NVIDIA Nemotron models, India’s technology leaders are accelerating productivity and efficiency across industries — from call centers to telecommunications and healthcare. Infosys, Persistent, Tech Mahindra and Wipro are leading the way for business transformation, improving back-office\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/india-enterprise-ai-agents/\">\n\t\tRead Article\t\t<span data-icon=\"y\"></span>\n\t</a>\n\t",
      "author": "John Fanelli",
      "source": "Nvidia Blog",
      "sourceType": "news",
      "pubDate": "Wed, 18 Feb 2026 00:30:41 +0000",
      "popularity": 0,
      "category": "inference-other"
    },
    {
      "title": "NVIDIA and Global Industrial Software Leaders Partner With India’s Largest Manufacturers to Drive AI Boom",
      "link": "https://blogs.nvidia.com/blog/india-global-industrial-software-leaders-manufacturers-ai/",
      "description": "India is entering a new age of industrialization, as AI transforms how the world designs, builds and runs physical products and systems. The country is investing $134 billion dollars in new manufacturing capacity across construction, automotive, renewable energy and robotics, creating both a massive challenge and opportunity to build software-defined factories from day one. At\t\n\t\tRead Article",
      "content": "India is entering a new age of industrialization, as AI transforms how the world designs, builds and runs physical products and systems. The country is investing $134 billion dollars in new manufacturing capacity across construction, automotive, renewable energy and robotics, creating both a massive challenge and opportunity to build software-defined factories from day one. At\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/india-global-industrial-software-leaders-manufacturers-ai/\">\n\t\tRead Article\t\t<span data-icon=\"y\"></span>\n\t</a>\n\t",
      "author": "Timothy Costa",
      "source": "Nvidia Blog",
      "sourceType": "news",
      "pubDate": "Wed, 18 Feb 2026 00:30:32 +0000",
      "popularity": 0,
      "category": "inference-other"
    },
    {
      "title": "Meta&#8217;s new deal with Nvidia buys up millions of AI chips",
      "link": "https://www.theverge.com/ai-artificial-intelligence/880513/nvidia-meta-ai-grace-vera-chips",
      "description": "Meta has struck a multiyear deal to expand its data centers with millions of Nvidia's Grace and Vera CPUs and Blackwell and Rubin GPUs. While Meta has long been using Nvidia's hardware for its AI products, this deal \"represents the first large-scale Nvidia Grace-only deployment,\" which Nvidia says will deliver \"significant performance-per-watt improvements in [Meta's] data centers.\" The deal also includes plans to add Nvidia's next-generation Vera CPUs to Meta's data centers in 2027. \nMeta is also working on its own in-house chips for running AI models, but according to the Financial Times, it has run into \"technical challenges and rollout  …\nRead the full story at The Verge.",
      "content": "\n\t\t\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\n<figure>\n\n<img alt=\"An illustration of the Meta logo\" data-caption=\"\" data-portal-copyright=\"Illustration by Nick Barclay / The Verge\" data-has-syndication-rights=\"1\" src=\"https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/STK043_VRG_Illo_N_Barclay_1_Meta-1.jpg?quality=90&#038;strip=all&#038;crop=0,0,100,100\" />\n\t<figcaption>\n\t\t</figcaption>\n</figure>\n<p class=\"has-text-align-none\">Meta has struck a multiyear deal to expand its data centers with millions of Nvidia's Grace and Vera CPUs and Blackwell and Rubin GPUs. While Meta has long been using Nvidia's hardware for its AI products, this deal \"represents the first large-scale Nvidia Grace-only deployment,\" which <a href=\"https://nvidianews.nvidia.com/news/meta-builds-ai-infrastructure-with-nvidia\">Nvidia says</a> will deliver \"significant performance-per-watt improvements in [Meta's] data centers.\" The deal also includes plans to add Nvidia's next-generation Vera CPUs to Meta's data centers in 2027. </p>\n<p class=\"has-text-align-none\">Meta is also <a href=\"https://www.theverge.com/2024/2/1/24058179/meta-reportedly-working-on-a-new-ai-chip-it-plans-to-launch-this-year\">working on its own in-house chips</a> for running AI models, but according to the <a href=\"https://www.ft.com/content/d3b50dfc-31fa-45a8-9184-c5f0476f4504\"><em>Financial Times</em></a>, it has run into \"technical challenges and rollout  …</p>\n<p><a href=\"https://www.theverge.com/ai-artificial-intelligence/880513/nvidia-meta-ai-grace-vera-chips\">Read the full story at The Verge.</a></p>\n\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t",
      "author": "Stevie Bonifield",
      "source": "The Verge AI",
      "sourceType": "news",
      "pubDate": "2026-02-18T00:27:08.000Z",
      "popularity": 0,
      "category": "architecture"
    },
    {
      "title": "India to Add 20,000 GPUs as AI Mission 2.0 Expands Compute and Chip Push",
      "link": "https://www.eetimes.com/india-to-add-20000-gpus-as-ai-mission-2-0-expands-compute-and-chip-push/",
      "description": "India ramps up its AI arsenal with 20,000 more GPUs under AI Mission 2.0.\nThe post India to Add 20,000 GPUs as AI Mission 2.0 Expands Compute and Chip Push appeared first on EE Times.",
      "content": "\n\t\t\t\t\t\t<p>India ramps up its AI arsenal with 20,000 more GPUs under AI Mission 2.0.</p>\n<p>The post <a href=\"https://www.eetimes.com/india-to-add-20000-gpus-as-ai-mission-2-0-expands-compute-and-chip-push/\">India to Add 20,000 GPUs as AI Mission 2.0 Expands Compute and Chip Push</a> appeared first on <a href=\"https://www.eetimes.com\">EE Times</a>.</p>\n\n\t\t\t\t\t",
      "author": "\n\t\t\t\t\t\tYashasvini Razdan\n\t\t\t\t\t",
      "source": "EE Times",
      "sourceType": "news",
      "pubDate": "Wed, 18 Feb 2026 00:00:00 +0000",
      "popularity": 0,
      "category": "inference-other"
    },
    {
      "title": " AMD denies report of MI455X delays as Nvidia VR200 systems are rumored to arrive early — company says Helios systems 'on target for 2H 2026' ",
      "link": "https://www.tomshardware.com/tech-industry/artificial-intelligence/amd-denies-report-of-mi455x-delays-as-nvidia-vr200-systems-are-rumored-to-arrive-early-company-says-helios-systems-on-target-for-2h-2026",
      "description": "AMD's Helios rack-scale solution based on the MI455X accelerators, the company's major hope for AI market, may slip to 2027, whereas Nvidia may speed up roll out of the Vera Rubin platform if the latest market rumors are to be believed.",
      "content": "\n                             AMD's Helios rack-scale solution based on the MI455X accelerators, the company's major hope for AI market, may slip to 2027, whereas Nvidia may speed up roll out of the Vera Rubin platform if the latest market rumors are to be believed. \n                                                                                                            ",
      "author": " Anton Shilov ",
      "source": "Tom's Hardware",
      "sourceType": "news",
      "pubDate": "Tue, 17 Feb 2026 20:56:10 +0000",
      "popularity": 0,
      "category": "inference-other"
    },
    {
      "title": "Nvidia China Gamble Meets Washington’s Regime",
      "link": "https://www.eetimes.com/nvidia-china-gamble-meets-washingtons-regime/",
      "description": "Nvidia and AMD face a volatile trade regime of tariffs and logistics hurdles for AI chips to China; U.S. Congress threatens export license revocation.\nThe post Nvidia China Gamble Meets Washington’s Regime appeared first on EE Times.",
      "content": "\n\t\t\t\t\t\t<p>Nvidia and AMD face a volatile trade regime of tariffs and logistics hurdles for AI chips to China; U.S. Congress threatens export license revocation.</p>\n<p>The post <a href=\"https://www.eetimes.com/nvidia-china-gamble-meets-washingtons-regime/\">Nvidia China Gamble Meets Washington’s Regime</a> appeared first on <a href=\"https://www.eetimes.com\">EE Times</a>.</p>\n\n\t\t\t\t\t",
      "author": "\n\t\t\t\t\t\tPablo Valerio\n\t\t\t\t\t",
      "source": "EE Times",
      "sourceType": "news",
      "pubDate": "Tue, 17 Feb 2026 19:10:06 +0000",
      "popularity": 0,
      "category": "inference-other"
    },
    {
      "title": " This Corsair RAM and Gigabyte AM5 motherboard bundle is just $67 more than buying the RAM alone – 32GB of RAM and B850 Aorus Elite Wifi 7 shaves $133 off buying separately ",
      "link": "https://www.tomshardware.com/pc-components/this-corsair-ram-and-gigabyte-am5-motherboard-bundle-is-just-usd67-more-than-buying-the-ram-alone-32gb-of-ram-and-b850-aorus-elite-wifi-7-shaves-usd133-off-buying-separately",
      "description": "Score 20% off at Newegg on a Gigabyte B850 Aorus Elite Wifi 7 + 32GB Corsair Vengeance RGB DDR5-6400 bundle—under $505 for AMD AM5 support, Wi-Fi 7, PCIe 5.0 M.2/slot, and eye-catching RGB.",
      "content": "\n                             Score 20% off at Newegg on a Gigabyte B850 Aorus Elite Wifi 7 + 32GB Corsair Vengeance RGB DDR5-6400 bundle—under $505 for AMD AM5 support, Wi-Fi 7, PCIe 5.0 M.2/slot, and eye-catching RGB. \n                                                                                                            ",
      "author": " Joe Shields ",
      "source": "Tom's Hardware",
      "sourceType": "news",
      "pubDate": "Tue, 17 Feb 2026 14:20:07 +0000",
      "popularity": 0,
      "category": "inference-other"
    },
    {
      "title": "Former Altera CEO Joins French AI Chip Startup",
      "link": "https://www.eetimes.com/former-altera-ceo-rivera-joins-french-ai-chip-startup-vsora/",
      "description": "Sandra Rivera joins Vsora to boost its AI chip game.\nThe post Former Altera CEO Joins French AI Chip Startup appeared first on EE Times.",
      "content": "\n\t\t\t\t\t\t<p>Sandra Rivera joins Vsora to boost its AI chip game.</p>\n<p>The post <a href=\"https://www.eetimes.com/former-altera-ceo-rivera-joins-french-ai-chip-startup-vsora/\">Former Altera CEO Joins French AI Chip Startup</a> appeared first on <a href=\"https://www.eetimes.com\">EE Times</a>.</p>\n\n\t\t\t\t\t",
      "author": "\n\t\t\t\t\t\tSally Ward-Foxton\n\t\t\t\t\t",
      "source": "EE Times",
      "sourceType": "news",
      "pubDate": "Tue, 17 Feb 2026 13:59:03 +0000",
      "popularity": 0,
      "category": "inference-other"
    },
    {
      "title": "ABI: A tightly integrated, unified, sparsity-aware, reconfigurable, compute near-register file/cache GPU architecture with light-weight softmax for deep learning, linear algebra, and Ising compute",
      "link": "https://arxiv.org/abs/2602.14262",
      "description": "arXiv:2602.14262v1 Announce Type: new \nAbstract: We present a tightly integrated and unified near-memory GPU architecture that delivers 6 to 16 times speedup and 6 to 13 times energy savings across Convolutional Neural Networks, Graph Convolutional Networks, Linear Programming, Large Language Models, and Ising workloads compared to MIAOW GPU. The design includes a custom sparsity-aware near-memory circuit providing about 1.5 times energy savings, and a lightweight softmax circuit providing about 1.6 times energy savings. The architecture supports reconfigurable compute up to INT16 with dynamic resolution updates and scales efficiently across problem sizes. ABI-enabled MI300 and Blackwell systems achieve about 4.5 times speedup over baseline MI300 and Blackwell.",
      "content": "arXiv:2602.14262v1 Announce Type: new \nAbstract: We present a tightly integrated and unified near-memory GPU architecture that delivers 6 to 16 times speedup and 6 to 13 times energy savings across Convolutional Neural Networks, Graph Convolutional Networks, Linear Programming, Large Language Models, and Ising workloads compared to MIAOW GPU. The design includes a custom sparsity-aware near-memory circuit providing about 1.5 times energy savings, and a lightweight softmax circuit providing about 1.6 times energy savings. The architecture supports reconfigurable compute up to INT16 with dynamic resolution updates and scales efficiently across problem sizes. ABI-enabled MI300 and Blackwell systems achieve about 4.5 times speedup over baseline MI300 and Blackwell.",
      "author": "Siddhartha Raman Sundara Raman, Jaydeep P. Kulkarni",
      "source": "arXiv Hardware Architecture",
      "sourceType": "news",
      "pubDate": "Tue, 17 Feb 2026 00:00:00 -0500",
      "popularity": 0,
      "category": "cloud-inference"
    },
    {
      "title": "Probabilistic approximate optimization using single-photon avalanche diode arrays",
      "link": "https://arxiv.org/abs/2602.13943",
      "description": "arXiv:2602.13943v1 Announce Type: cross \nAbstract: Combinatorial optimization problems are central to science and engineering and specialized hardware from quantum annealers to classical Ising machines are being actively developed to address them. These systems typically sample from a fixed energy landscape defined by the problem Hamiltonian encoding the discrete optimization problem. The recently introduced Probabilistic Approximate Optimization Algorithm (PAOA) takes a different approach: it treats the optimization landscape itself as variational, iteratively learning circuit parameters from samples. Here, we demonstrate PAOA on a 64$\\times$64 perimeter-gated single-photon avalanche diode (pgSPAD) array fabricated in 0.35 $\\mu$m CMOS, the first realization of the algorithm using intrinsically stochastic nanodevices. Each p-bit exhibits a device-specific, asymmetric (Gompertz-type) activation function due to dark-count variability. Rather than calibrating devices to enforce a uniform symmetric (logistic/tanh) activation, PAOA learns around device variations, absorbing residual activation and other mismatches into the variational parameters. On canonical 26-spin Sherrington-Kirkpatrick instances, PAOA achieves high approximation ratios with $2p$ parameters ($p$ up to 17 layers), and pgSPAD-based inference closely tracks CPU simulations. These results show that variational learning can accommodate the non-idealities inherent to nanoscale devices, suggesting a practical path toward larger-scale, CMOS-compatible probabilistic computers.",
      "content": "arXiv:2602.13943v1 Announce Type: cross \nAbstract: Combinatorial optimization problems are central to science and engineering and specialized hardware from quantum annealers to classical Ising machines are being actively developed to address them. These systems typically sample from a fixed energy landscape defined by the problem Hamiltonian encoding the discrete optimization problem. The recently introduced Probabilistic Approximate Optimization Algorithm (PAOA) takes a different approach: it treats the optimization landscape itself as variational, iteratively learning circuit parameters from samples. Here, we demonstrate PAOA on a 64$\\times$64 perimeter-gated single-photon avalanche diode (pgSPAD) array fabricated in 0.35 $\\mu$m CMOS, the first realization of the algorithm using intrinsically stochastic nanodevices. Each p-bit exhibits a device-specific, asymmetric (Gompertz-type) activation function due to dark-count variability. Rather than calibrating devices to enforce a uniform symmetric (logistic/tanh) activation, PAOA learns around device variations, absorbing residual activation and other mismatches into the variational parameters. On canonical 26-spin Sherrington-Kirkpatrick instances, PAOA achieves high approximation ratios with $2p$ parameters ($p$ up to 17 layers), and pgSPAD-based inference closely tracks CPU simulations. These results show that variational learning can accommodate the non-idealities inherent to nanoscale devices, suggesting a practical path toward larger-scale, CMOS-compatible probabilistic computers.",
      "author": "Ziyad Alsawidan, Abdelrahman S. Abdelrahman, Md Sakibur Sajal, Shuvro Chowdhury, Kai-Chun Lin, Hunter Guthrie, Sanjay Seshan, Shawn Blanton, Flaviano Morone, Marc Dandin, Kerem Y. Camsari, Tathagata Srimani",
      "source": "arXiv Hardware Architecture",
      "sourceType": "news",
      "pubDate": "Tue, 17 Feb 2026 00:00:00 -0500",
      "popularity": 0,
      "category": "inference-optimization"
    },
    {
      "title": "RNM-TD3: N:M Semi-structured Sparse Reinforcement Learning From Scratch",
      "link": "https://arxiv.org/abs/2602.14578",
      "description": "arXiv:2602.14578v1 Announce Type: cross \nAbstract: Sparsity is a well-studied technique for compressing deep neural networks (DNNs) without compromising performance. In deep reinforcement learning (DRL), neural networks with up to 5% of their original weights can still be trained with minimal performance loss compared to their dense counterparts. However, most existing methods rely on unstructured fine-grained sparsity, which limits hardware acceleration opportunities due to irregular computation patterns. Structured coarse-grained sparsity enables hardware acceleration, yet typically degrades performance and increases pruning complexity. In this work, we present, to the best of our knowledge, the first study on N:M structured sparsity in RL, which balances compression, performance, and hardware efficiency. Our framework enforces row-wise N:M sparsity throughout training for all networks in off-policy RL (TD3), maintaining compatibility with accelerators that support N:M sparse matrix operations. Experiments on continuous-control benchmarks show that RNM-TD3, our N:M sparse agent, outperforms its dense counterpart at 50%-75% sparsity (e.g., 2:4 and 1:4), achieving up to a 14% increase in performance at 2:4 sparsity on the Ant environment. RNM-TD3 remains competitive even at 87.5% sparsity (1:8), while enabling potential training speedups.",
      "content": "arXiv:2602.14578v1 Announce Type: cross \nAbstract: Sparsity is a well-studied technique for compressing deep neural networks (DNNs) without compromising performance. In deep reinforcement learning (DRL), neural networks with up to 5% of their original weights can still be trained with minimal performance loss compared to their dense counterparts. However, most existing methods rely on unstructured fine-grained sparsity, which limits hardware acceleration opportunities due to irregular computation patterns. Structured coarse-grained sparsity enables hardware acceleration, yet typically degrades performance and increases pruning complexity. In this work, we present, to the best of our knowledge, the first study on N:M structured sparsity in RL, which balances compression, performance, and hardware efficiency. Our framework enforces row-wise N:M sparsity throughout training for all networks in off-policy RL (TD3), maintaining compatibility with accelerators that support N:M sparse matrix operations. Experiments on continuous-control benchmarks show that RNM-TD3, our N:M sparse agent, outperforms its dense counterpart at 50%-75% sparsity (e.g., 2:4 and 1:4), achieving up to a 14% increase in performance at 2:4 sparsity on the Ant environment. RNM-TD3 remains competitive even at 87.5% sparsity (1:8), while enabling potential training speedups.",
      "author": "Isam Vrce, Andreas Kassler, G\\\"ok\\c{c}e Aydos",
      "source": "arXiv Hardware Architecture",
      "sourceType": "news",
      "pubDate": "Tue, 17 Feb 2026 00:00:00 -0500",
      "popularity": 0,
      "category": "inference-optimization"
    },
    {
      "title": "Optimizing Task Scheduling in Fog Computing with Deadline Awareness",
      "link": "https://arxiv.org/abs/2509.07378",
      "description": "arXiv:2509.07378v5 Announce Type: replace-cross \nAbstract: The rise of Internet of Things (IoT) devices has led to the development of numerous time-sensitive applications that require quick responses and low latency. Fog computing has emerged as a solution for processing these IoT applications, but it faces challenges such as resource allocation and job scheduling. Therefore, it is crucial to determine how to assign and schedule tasks on Fog nodes. This work aims to schedule tasks in IoT while minimizing the total energy consumption of nodes and enhancing the Quality of Service (QoS) requirements of IoT tasks, taking into account task deadlines. This paper classifies Fog nodes into two categories based on their traffic level: low and high. It schedules short-deadline tasks on low-traffic nodes using an Improved Golden Eagle Optimization (IGEO) algorithm, an enhancement that utilizes genetic operators for discretization. Long-deadline tasks are processed on high-traffic nodes using reinforcement learning (RL). This combined approach is called the Reinforcement Improved Golden Eagle Optimization (RIGEO) algorithm. Experimental results demonstrate that RIGEO achieves up to a 29% reduction in energy consumption, up to an 86% improvement in response time, and up to a 19% reduction in deadline violations compared to state-of-the-art algorithms.",
      "content": "arXiv:2509.07378v5 Announce Type: replace-cross \nAbstract: The rise of Internet of Things (IoT) devices has led to the development of numerous time-sensitive applications that require quick responses and low latency. Fog computing has emerged as a solution for processing these IoT applications, but it faces challenges such as resource allocation and job scheduling. Therefore, it is crucial to determine how to assign and schedule tasks on Fog nodes. This work aims to schedule tasks in IoT while minimizing the total energy consumption of nodes and enhancing the Quality of Service (QoS) requirements of IoT tasks, taking into account task deadlines. This paper classifies Fog nodes into two categories based on their traffic level: low and high. It schedules short-deadline tasks on low-traffic nodes using an Improved Golden Eagle Optimization (IGEO) algorithm, an enhancement that utilizes genetic operators for discretization. Long-deadline tasks are processed on high-traffic nodes using reinforcement learning (RL). This combined approach is called the Reinforcement Improved Golden Eagle Optimization (RIGEO) algorithm. Experimental results demonstrate that RIGEO achieves up to a 29% reduction in energy consumption, up to an 86% improvement in response time, and up to a 19% reduction in deadline violations compared to state-of-the-art algorithms.",
      "author": "Mohammad Sadegh Sirjani, Mohammad Ahmad, Amir Mousavi, Erfan Nourbakhsh, Khoa Nguyen",
      "source": "arXiv Hardware Architecture",
      "sourceType": "news",
      "pubDate": "Tue, 17 Feb 2026 00:00:00 -0500",
      "popularity": 0,
      "category": "inference-optimization"
    },
    {
      "title": "Scope: A Scalable Merged Pipeline Framework for Multi-Chip-Module NN Accelerators",
      "link": "https://arxiv.org/abs/2602.14393",
      "description": "arXiv:2602.14393v1 Announce Type: new \nAbstract: Neural network (NN) accelerators with multi-chip-module (MCM) architectures enable integration of massive computation capability; however, they face challenges of computing resource underutilization and off-chip communication overheads. Traditional parallelization schemes for NN inference on MCM architectures, such as intra-layer parallelism and inter-layer pipelining, show incompetency in breaking through both challenges, limiting the scalability of MCM architectures.\n  We observed that existing works typically deploy layers separately rather than considering them jointly. This underexploited dimension leads to compromises between system computation and communication, thus hindering optimal utilization, especially as hardware/software scale. To address this limitation, we propose Scope, a merged pipeline framework incorporating this overlooked multi-layer dimension, thereby achieving improved throughput and scalability by relaxing tradeoffs between computation, communication and memory costs. This new dimension, however, adds to the complexity of design space exploration (DSE). To tackle this, we develop a series of search algorithms that achieves exponential-to-linear complexity reduction, while identifying solutions that rank in the top 0.05% of performance. Experiments show that Scope achieves up to 1.73x throughput improvement while maintaining similar energy consumption for ResNet-152 inference compared to state-of-the-art approaches.",
      "content": "arXiv:2602.14393v1 Announce Type: new \nAbstract: Neural network (NN) accelerators with multi-chip-module (MCM) architectures enable integration of massive computation capability; however, they face challenges of computing resource underutilization and off-chip communication overheads. Traditional parallelization schemes for NN inference on MCM architectures, such as intra-layer parallelism and inter-layer pipelining, show incompetency in breaking through both challenges, limiting the scalability of MCM architectures.\n  We observed that existing works typically deploy layers separately rather than considering them jointly. This underexploited dimension leads to compromises between system computation and communication, thus hindering optimal utilization, especially as hardware/software scale. To address this limitation, we propose Scope, a merged pipeline framework incorporating this overlooked multi-layer dimension, thereby achieving improved throughput and scalability by relaxing tradeoffs between computation, communication and memory costs. This new dimension, however, adds to the complexity of design space exploration (DSE). To tackle this, we develop a series of search algorithms that achieves exponential-to-linear complexity reduction, while identifying solutions that rank in the top 0.05% of performance. Experiments show that Scope achieves up to 1.73x throughput improvement while maintaining similar energy consumption for ResNet-152 inference compared to state-of-the-art approaches.",
      "author": "Zongle Huang, Hongyang Jia, Kaiwei Zou, Yongpan Liu",
      "source": "arXiv Hardware Architecture",
      "sourceType": "news",
      "pubDate": "Tue, 17 Feb 2026 00:00:00 -0500",
      "popularity": 0,
      "category": "architecture"
    },
    {
      "title": "TrackCore-F: Deploying Transformer-Based Subatomic Particle Tracking on FPGAs",
      "link": "https://arxiv.org/abs/2509.26335",
      "description": "arXiv:2509.26335v2 Announce Type: replace-cross \nAbstract: The Transformer Machine Learning (ML) architecture has been gaining considerable momentum in recent years. In particular, computational High-Energy Physics tasks such as jet tagging and particle track reconstruction (tracking), have either achieved proper solutions, or reached considerable milestones using Transformers. On the other hand, the use of specialised hardware accelerators, especially FPGAs, is an effective method to achieve online, or pseudo-online latencies. The development and integration of Transformer-based ML to FPGAs is still ongoing and the support from current tools is very limited or non-existent. Additionally, FPGA resources present a significant constraint. Considering the model size alone, while smaller models can be deployed directly, larger models are to be partitioned in a meaningful and ideally, automated way. We aim to develop methodologies and tools for monolithic, or partitioned Transformer synthesis, specifically targeting inference. Our primary use-case involves two machine learning model designs for tracking, derived from the TrackFormers project. We elaborate our development approach, present preliminary results, and provide comparisons.",
      "content": "arXiv:2509.26335v2 Announce Type: replace-cross \nAbstract: The Transformer Machine Learning (ML) architecture has been gaining considerable momentum in recent years. In particular, computational High-Energy Physics tasks such as jet tagging and particle track reconstruction (tracking), have either achieved proper solutions, or reached considerable milestones using Transformers. On the other hand, the use of specialised hardware accelerators, especially FPGAs, is an effective method to achieve online, or pseudo-online latencies. The development and integration of Transformer-based ML to FPGAs is still ongoing and the support from current tools is very limited or non-existent. Additionally, FPGA resources present a significant constraint. Considering the model size alone, while smaller models can be deployed directly, larger models are to be partitioned in a meaningful and ideally, automated way. We aim to develop methodologies and tools for monolithic, or partitioned Transformer synthesis, specifically targeting inference. Our primary use-case involves two machine learning model designs for tracking, derived from the TrackFormers project. We elaborate our development approach, present preliminary results, and provide comparisons.",
      "author": "Arjan Blankestijn, Uraz Odyurt, Amirreza Yousefzadeh",
      "source": "arXiv Hardware Architecture",
      "sourceType": "news",
      "pubDate": "Tue, 17 Feb 2026 00:00:00 -0500",
      "popularity": 0,
      "category": "architecture"
    }
  ],
  "history": [
    {
      "title": "India Fuels Its AI Mission With NVIDIA",
      "link": "https://blogs.nvidia.com/blog/india-ai-mission-infrastructure-models/",
      "description": "From AI infrastructure leaders to frontier model developers, India is teaming with NVIDIA to drive AI transformation across the nation.",
      "content": "From AI infrastructure leaders to frontier model developers, India is teaming with NVIDIA to drive AI transformation across the nation.",
      "author": "Jay Puri",
      "source": "Nvidia Blog",
      "sourceType": "news",
      "pubDate": "Wed, 18 Feb 2026 00:30:49 +0000",
      "popularity": 0,
      "category": "inference-other"
    },
    {
      "title": "India’s Global Systems Integrators Build Next Wave of Enterprise Agents With NVIDIA AI, Transforming Back Office and Customer Support",
      "link": "https://blogs.nvidia.com/blog/india-enterprise-ai-agents/",
      "description": "Agentic AI is reshaping India’s tech industry, delivering leaps in services worldwide. Tapping into NVIDIA AI Enterprise software and NVIDIA Nemotron models, India’s technology leaders are accelerating productivity and efficiency across industries — from call centers to telecommunications and healthcare. Infosys, Persistent, Tech Mahindra and Wipro are leading the way for business transformation, improving back-office\t\n\t\tRead Article",
      "content": "Agentic AI is reshaping India’s tech industry, delivering leaps in services worldwide. Tapping into NVIDIA AI Enterprise software and NVIDIA Nemotron models, India’s technology leaders are accelerating productivity and efficiency across industries — from call centers to telecommunications and healthcare. Infosys, Persistent, Tech Mahindra and Wipro are leading the way for business transformation, improving back-office\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/india-enterprise-ai-agents/\">\n\t\tRead Article\t\t<span data-icon=\"y\"></span>\n\t</a>\n\t",
      "author": "John Fanelli",
      "source": "Nvidia Blog",
      "sourceType": "news",
      "pubDate": "Wed, 18 Feb 2026 00:30:41 +0000",
      "popularity": 0,
      "category": "inference-other"
    },
    {
      "title": "NVIDIA and Global Industrial Software Leaders Partner With India’s Largest Manufacturers to Drive AI Boom",
      "link": "https://blogs.nvidia.com/blog/india-global-industrial-software-leaders-manufacturers-ai/",
      "description": "India is entering a new age of industrialization, as AI transforms how the world designs, builds and runs physical products and systems. The country is investing $134 billion dollars in new manufacturing capacity across construction, automotive, renewable energy and robotics, creating both a massive challenge and opportunity to build software-defined factories from day one. At\t\n\t\tRead Article",
      "content": "India is entering a new age of industrialization, as AI transforms how the world designs, builds and runs physical products and systems. The country is investing $134 billion dollars in new manufacturing capacity across construction, automotive, renewable energy and robotics, creating both a massive challenge and opportunity to build software-defined factories from day one. At\t<a class=\"read-more\" href=\"https://blogs.nvidia.com/blog/india-global-industrial-software-leaders-manufacturers-ai/\">\n\t\tRead Article\t\t<span data-icon=\"y\"></span>\n\t</a>\n\t",
      "author": "Timothy Costa",
      "source": "Nvidia Blog",
      "sourceType": "news",
      "pubDate": "Wed, 18 Feb 2026 00:30:32 +0000",
      "popularity": 0,
      "category": "inference-other"
    },
    {
      "title": "Meta&#8217;s new deal with Nvidia buys up millions of AI chips",
      "link": "https://www.theverge.com/ai-artificial-intelligence/880513/nvidia-meta-ai-grace-vera-chips",
      "description": "Meta has struck a multiyear deal to expand its data centers with millions of Nvidia's Grace and Vera CPUs and Blackwell and Rubin GPUs. While Meta has long been using Nvidia's hardware for its AI products, this deal \"represents the first large-scale Nvidia Grace-only deployment,\" which Nvidia says will deliver \"significant performance-per-watt improvements in [Meta's] data centers.\" The deal also includes plans to add Nvidia's next-generation Vera CPUs to Meta's data centers in 2027. \nMeta is also working on its own in-house chips for running AI models, but according to the Financial Times, it has run into \"technical challenges and rollout  …\nRead the full story at The Verge.",
      "content": "\n\t\t\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\n<figure>\n\n<img alt=\"An illustration of the Meta logo\" data-caption=\"\" data-portal-copyright=\"Illustration by Nick Barclay / The Verge\" data-has-syndication-rights=\"1\" src=\"https://platform.theverge.com/wp-content/uploads/sites/2/2025/10/STK043_VRG_Illo_N_Barclay_1_Meta-1.jpg?quality=90&#038;strip=all&#038;crop=0,0,100,100\" />\n\t<figcaption>\n\t\t</figcaption>\n</figure>\n<p class=\"has-text-align-none\">Meta has struck a multiyear deal to expand its data centers with millions of Nvidia's Grace and Vera CPUs and Blackwell and Rubin GPUs. While Meta has long been using Nvidia's hardware for its AI products, this deal \"represents the first large-scale Nvidia Grace-only deployment,\" which <a href=\"https://nvidianews.nvidia.com/news/meta-builds-ai-infrastructure-with-nvidia\">Nvidia says</a> will deliver \"significant performance-per-watt improvements in [Meta's] data centers.\" The deal also includes plans to add Nvidia's next-generation Vera CPUs to Meta's data centers in 2027. </p>\n<p class=\"has-text-align-none\">Meta is also <a href=\"https://www.theverge.com/2024/2/1/24058179/meta-reportedly-working-on-a-new-ai-chip-it-plans-to-launch-this-year\">working on its own in-house chips</a> for running AI models, but according to the <a href=\"https://www.ft.com/content/d3b50dfc-31fa-45a8-9184-c5f0476f4504\"><em>Financial Times</em></a>, it has run into \"technical challenges and rollout  …</p>\n<p><a href=\"https://www.theverge.com/ai-artificial-intelligence/880513/nvidia-meta-ai-grace-vera-chips\">Read the full story at The Verge.</a></p>\n\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t",
      "author": "Stevie Bonifield",
      "source": "The Verge AI",
      "sourceType": "news",
      "pubDate": "2026-02-18T00:27:08.000Z",
      "popularity": 0,
      "category": "architecture"
    },
    {
      "title": "India to Add 20,000 GPUs as AI Mission 2.0 Expands Compute and Chip Push",
      "link": "https://www.eetimes.com/india-to-add-20000-gpus-as-ai-mission-2-0-expands-compute-and-chip-push/",
      "description": "India ramps up its AI arsenal with 20,000 more GPUs under AI Mission 2.0.\nThe post India to Add 20,000 GPUs as AI Mission 2.0 Expands Compute and Chip Push appeared first on EE Times.",
      "content": "\n\t\t\t\t\t\t<p>India ramps up its AI arsenal with 20,000 more GPUs under AI Mission 2.0.</p>\n<p>The post <a href=\"https://www.eetimes.com/india-to-add-20000-gpus-as-ai-mission-2-0-expands-compute-and-chip-push/\">India to Add 20,000 GPUs as AI Mission 2.0 Expands Compute and Chip Push</a> appeared first on <a href=\"https://www.eetimes.com\">EE Times</a>.</p>\n\n\t\t\t\t\t",
      "author": "\n\t\t\t\t\t\tYashasvini Razdan\n\t\t\t\t\t",
      "source": "EE Times",
      "sourceType": "news",
      "pubDate": "Wed, 18 Feb 2026 00:00:00 +0000",
      "popularity": 0,
      "category": "inference-other"
    },
    {
      "title": " AMD denies report of MI455X delays as Nvidia VR200 systems are rumored to arrive early — company says Helios systems 'on target for 2H 2026' ",
      "link": "https://www.tomshardware.com/tech-industry/artificial-intelligence/amd-denies-report-of-mi455x-delays-as-nvidia-vr200-systems-are-rumored-to-arrive-early-company-says-helios-systems-on-target-for-2h-2026",
      "description": "AMD's Helios rack-scale solution based on the MI455X accelerators, the company's major hope for AI market, may slip to 2027, whereas Nvidia may speed up roll out of the Vera Rubin platform if the latest market rumors are to be believed.",
      "content": "\n                             AMD's Helios rack-scale solution based on the MI455X accelerators, the company's major hope for AI market, may slip to 2027, whereas Nvidia may speed up roll out of the Vera Rubin platform if the latest market rumors are to be believed. \n                                                                                                            ",
      "author": " Anton Shilov ",
      "source": "Tom's Hardware",
      "sourceType": "news",
      "pubDate": "Tue, 17 Feb 2026 20:56:10 +0000",
      "popularity": 0,
      "category": "inference-other"
    },
    {
      "title": "Nvidia China Gamble Meets Washington’s Regime",
      "link": "https://www.eetimes.com/nvidia-china-gamble-meets-washingtons-regime/",
      "description": "Nvidia and AMD face a volatile trade regime of tariffs and logistics hurdles for AI chips to China; U.S. Congress threatens export license revocation.\nThe post Nvidia China Gamble Meets Washington’s Regime appeared first on EE Times.",
      "content": "\n\t\t\t\t\t\t<p>Nvidia and AMD face a volatile trade regime of tariffs and logistics hurdles for AI chips to China; U.S. Congress threatens export license revocation.</p>\n<p>The post <a href=\"https://www.eetimes.com/nvidia-china-gamble-meets-washingtons-regime/\">Nvidia China Gamble Meets Washington’s Regime</a> appeared first on <a href=\"https://www.eetimes.com\">EE Times</a>.</p>\n\n\t\t\t\t\t",
      "author": "\n\t\t\t\t\t\tPablo Valerio\n\t\t\t\t\t",
      "source": "EE Times",
      "sourceType": "news",
      "pubDate": "Tue, 17 Feb 2026 19:10:06 +0000",
      "popularity": 0,
      "category": "inference-other"
    },
    {
      "title": " This Corsair RAM and Gigabyte AM5 motherboard bundle is just $67 more than buying the RAM alone – 32GB of RAM and B850 Aorus Elite Wifi 7 shaves $133 off buying separately ",
      "link": "https://www.tomshardware.com/pc-components/this-corsair-ram-and-gigabyte-am5-motherboard-bundle-is-just-usd67-more-than-buying-the-ram-alone-32gb-of-ram-and-b850-aorus-elite-wifi-7-shaves-usd133-off-buying-separately",
      "description": "Score 20% off at Newegg on a Gigabyte B850 Aorus Elite Wifi 7 + 32GB Corsair Vengeance RGB DDR5-6400 bundle—under $505 for AMD AM5 support, Wi-Fi 7, PCIe 5.0 M.2/slot, and eye-catching RGB.",
      "content": "\n                             Score 20% off at Newegg on a Gigabyte B850 Aorus Elite Wifi 7 + 32GB Corsair Vengeance RGB DDR5-6400 bundle—under $505 for AMD AM5 support, Wi-Fi 7, PCIe 5.0 M.2/slot, and eye-catching RGB. \n                                                                                                            ",
      "author": " Joe Shields ",
      "source": "Tom's Hardware",
      "sourceType": "news",
      "pubDate": "Tue, 17 Feb 2026 14:20:07 +0000",
      "popularity": 0,
      "category": "inference-other"
    },
    {
      "title": "Former Altera CEO Joins French AI Chip Startup",
      "link": "https://www.eetimes.com/former-altera-ceo-rivera-joins-french-ai-chip-startup-vsora/",
      "description": "Sandra Rivera joins Vsora to boost its AI chip game.\nThe post Former Altera CEO Joins French AI Chip Startup appeared first on EE Times.",
      "content": "\n\t\t\t\t\t\t<p>Sandra Rivera joins Vsora to boost its AI chip game.</p>\n<p>The post <a href=\"https://www.eetimes.com/former-altera-ceo-rivera-joins-french-ai-chip-startup-vsora/\">Former Altera CEO Joins French AI Chip Startup</a> appeared first on <a href=\"https://www.eetimes.com\">EE Times</a>.</p>\n\n\t\t\t\t\t",
      "author": "\n\t\t\t\t\t\tSally Ward-Foxton\n\t\t\t\t\t",
      "source": "EE Times",
      "sourceType": "news",
      "pubDate": "Tue, 17 Feb 2026 13:59:03 +0000",
      "popularity": 0,
      "category": "inference-other"
    },
    {
      "title": "ABI: A tightly integrated, unified, sparsity-aware, reconfigurable, compute near-register file/cache GPU architecture with light-weight softmax for deep learning, linear algebra, and Ising compute",
      "link": "https://arxiv.org/abs/2602.14262",
      "description": "arXiv:2602.14262v1 Announce Type: new \nAbstract: We present a tightly integrated and unified near-memory GPU architecture that delivers 6 to 16 times speedup and 6 to 13 times energy savings across Convolutional Neural Networks, Graph Convolutional Networks, Linear Programming, Large Language Models, and Ising workloads compared to MIAOW GPU. The design includes a custom sparsity-aware near-memory circuit providing about 1.5 times energy savings, and a lightweight softmax circuit providing about 1.6 times energy savings. The architecture supports reconfigurable compute up to INT16 with dynamic resolution updates and scales efficiently across problem sizes. ABI-enabled MI300 and Blackwell systems achieve about 4.5 times speedup over baseline MI300 and Blackwell.",
      "content": "arXiv:2602.14262v1 Announce Type: new \nAbstract: We present a tightly integrated and unified near-memory GPU architecture that delivers 6 to 16 times speedup and 6 to 13 times energy savings across Convolutional Neural Networks, Graph Convolutional Networks, Linear Programming, Large Language Models, and Ising workloads compared to MIAOW GPU. The design includes a custom sparsity-aware near-memory circuit providing about 1.5 times energy savings, and a lightweight softmax circuit providing about 1.6 times energy savings. The architecture supports reconfigurable compute up to INT16 with dynamic resolution updates and scales efficiently across problem sizes. ABI-enabled MI300 and Blackwell systems achieve about 4.5 times speedup over baseline MI300 and Blackwell.",
      "author": "Siddhartha Raman Sundara Raman, Jaydeep P. Kulkarni",
      "source": "arXiv Hardware Architecture",
      "sourceType": "news",
      "pubDate": "Tue, 17 Feb 2026 00:00:00 -0500",
      "popularity": 0,
      "category": "cloud-inference"
    },
    {
      "title": "Probabilistic approximate optimization using single-photon avalanche diode arrays",
      "link": "https://arxiv.org/abs/2602.13943",
      "description": "arXiv:2602.13943v1 Announce Type: cross \nAbstract: Combinatorial optimization problems are central to science and engineering and specialized hardware from quantum annealers to classical Ising machines are being actively developed to address them. These systems typically sample from a fixed energy landscape defined by the problem Hamiltonian encoding the discrete optimization problem. The recently introduced Probabilistic Approximate Optimization Algorithm (PAOA) takes a different approach: it treats the optimization landscape itself as variational, iteratively learning circuit parameters from samples. Here, we demonstrate PAOA on a 64$\\times$64 perimeter-gated single-photon avalanche diode (pgSPAD) array fabricated in 0.35 $\\mu$m CMOS, the first realization of the algorithm using intrinsically stochastic nanodevices. Each p-bit exhibits a device-specific, asymmetric (Gompertz-type) activation function due to dark-count variability. Rather than calibrating devices to enforce a uniform symmetric (logistic/tanh) activation, PAOA learns around device variations, absorbing residual activation and other mismatches into the variational parameters. On canonical 26-spin Sherrington-Kirkpatrick instances, PAOA achieves high approximation ratios with $2p$ parameters ($p$ up to 17 layers), and pgSPAD-based inference closely tracks CPU simulations. These results show that variational learning can accommodate the non-idealities inherent to nanoscale devices, suggesting a practical path toward larger-scale, CMOS-compatible probabilistic computers.",
      "content": "arXiv:2602.13943v1 Announce Type: cross \nAbstract: Combinatorial optimization problems are central to science and engineering and specialized hardware from quantum annealers to classical Ising machines are being actively developed to address them. These systems typically sample from a fixed energy landscape defined by the problem Hamiltonian encoding the discrete optimization problem. The recently introduced Probabilistic Approximate Optimization Algorithm (PAOA) takes a different approach: it treats the optimization landscape itself as variational, iteratively learning circuit parameters from samples. Here, we demonstrate PAOA on a 64$\\times$64 perimeter-gated single-photon avalanche diode (pgSPAD) array fabricated in 0.35 $\\mu$m CMOS, the first realization of the algorithm using intrinsically stochastic nanodevices. Each p-bit exhibits a device-specific, asymmetric (Gompertz-type) activation function due to dark-count variability. Rather than calibrating devices to enforce a uniform symmetric (logistic/tanh) activation, PAOA learns around device variations, absorbing residual activation and other mismatches into the variational parameters. On canonical 26-spin Sherrington-Kirkpatrick instances, PAOA achieves high approximation ratios with $2p$ parameters ($p$ up to 17 layers), and pgSPAD-based inference closely tracks CPU simulations. These results show that variational learning can accommodate the non-idealities inherent to nanoscale devices, suggesting a practical path toward larger-scale, CMOS-compatible probabilistic computers.",
      "author": "Ziyad Alsawidan, Abdelrahman S. Abdelrahman, Md Sakibur Sajal, Shuvro Chowdhury, Kai-Chun Lin, Hunter Guthrie, Sanjay Seshan, Shawn Blanton, Flaviano Morone, Marc Dandin, Kerem Y. Camsari, Tathagata Srimani",
      "source": "arXiv Hardware Architecture",
      "sourceType": "news",
      "pubDate": "Tue, 17 Feb 2026 00:00:00 -0500",
      "popularity": 0,
      "category": "inference-optimization"
    },
    {
      "title": "RNM-TD3: N:M Semi-structured Sparse Reinforcement Learning From Scratch",
      "link": "https://arxiv.org/abs/2602.14578",
      "description": "arXiv:2602.14578v1 Announce Type: cross \nAbstract: Sparsity is a well-studied technique for compressing deep neural networks (DNNs) without compromising performance. In deep reinforcement learning (DRL), neural networks with up to 5% of their original weights can still be trained with minimal performance loss compared to their dense counterparts. However, most existing methods rely on unstructured fine-grained sparsity, which limits hardware acceleration opportunities due to irregular computation patterns. Structured coarse-grained sparsity enables hardware acceleration, yet typically degrades performance and increases pruning complexity. In this work, we present, to the best of our knowledge, the first study on N:M structured sparsity in RL, which balances compression, performance, and hardware efficiency. Our framework enforces row-wise N:M sparsity throughout training for all networks in off-policy RL (TD3), maintaining compatibility with accelerators that support N:M sparse matrix operations. Experiments on continuous-control benchmarks show that RNM-TD3, our N:M sparse agent, outperforms its dense counterpart at 50%-75% sparsity (e.g., 2:4 and 1:4), achieving up to a 14% increase in performance at 2:4 sparsity on the Ant environment. RNM-TD3 remains competitive even at 87.5% sparsity (1:8), while enabling potential training speedups.",
      "content": "arXiv:2602.14578v1 Announce Type: cross \nAbstract: Sparsity is a well-studied technique for compressing deep neural networks (DNNs) without compromising performance. In deep reinforcement learning (DRL), neural networks with up to 5% of their original weights can still be trained with minimal performance loss compared to their dense counterparts. However, most existing methods rely on unstructured fine-grained sparsity, which limits hardware acceleration opportunities due to irregular computation patterns. Structured coarse-grained sparsity enables hardware acceleration, yet typically degrades performance and increases pruning complexity. In this work, we present, to the best of our knowledge, the first study on N:M structured sparsity in RL, which balances compression, performance, and hardware efficiency. Our framework enforces row-wise N:M sparsity throughout training for all networks in off-policy RL (TD3), maintaining compatibility with accelerators that support N:M sparse matrix operations. Experiments on continuous-control benchmarks show that RNM-TD3, our N:M sparse agent, outperforms its dense counterpart at 50%-75% sparsity (e.g., 2:4 and 1:4), achieving up to a 14% increase in performance at 2:4 sparsity on the Ant environment. RNM-TD3 remains competitive even at 87.5% sparsity (1:8), while enabling potential training speedups.",
      "author": "Isam Vrce, Andreas Kassler, G\\\"ok\\c{c}e Aydos",
      "source": "arXiv Hardware Architecture",
      "sourceType": "news",
      "pubDate": "Tue, 17 Feb 2026 00:00:00 -0500",
      "popularity": 0,
      "category": "inference-optimization"
    },
    {
      "title": "Optimizing Task Scheduling in Fog Computing with Deadline Awareness",
      "link": "https://arxiv.org/abs/2509.07378",
      "description": "arXiv:2509.07378v5 Announce Type: replace-cross \nAbstract: The rise of Internet of Things (IoT) devices has led to the development of numerous time-sensitive applications that require quick responses and low latency. Fog computing has emerged as a solution for processing these IoT applications, but it faces challenges such as resource allocation and job scheduling. Therefore, it is crucial to determine how to assign and schedule tasks on Fog nodes. This work aims to schedule tasks in IoT while minimizing the total energy consumption of nodes and enhancing the Quality of Service (QoS) requirements of IoT tasks, taking into account task deadlines. This paper classifies Fog nodes into two categories based on their traffic level: low and high. It schedules short-deadline tasks on low-traffic nodes using an Improved Golden Eagle Optimization (IGEO) algorithm, an enhancement that utilizes genetic operators for discretization. Long-deadline tasks are processed on high-traffic nodes using reinforcement learning (RL). This combined approach is called the Reinforcement Improved Golden Eagle Optimization (RIGEO) algorithm. Experimental results demonstrate that RIGEO achieves up to a 29% reduction in energy consumption, up to an 86% improvement in response time, and up to a 19% reduction in deadline violations compared to state-of-the-art algorithms.",
      "content": "arXiv:2509.07378v5 Announce Type: replace-cross \nAbstract: The rise of Internet of Things (IoT) devices has led to the development of numerous time-sensitive applications that require quick responses and low latency. Fog computing has emerged as a solution for processing these IoT applications, but it faces challenges such as resource allocation and job scheduling. Therefore, it is crucial to determine how to assign and schedule tasks on Fog nodes. This work aims to schedule tasks in IoT while minimizing the total energy consumption of nodes and enhancing the Quality of Service (QoS) requirements of IoT tasks, taking into account task deadlines. This paper classifies Fog nodes into two categories based on their traffic level: low and high. It schedules short-deadline tasks on low-traffic nodes using an Improved Golden Eagle Optimization (IGEO) algorithm, an enhancement that utilizes genetic operators for discretization. Long-deadline tasks are processed on high-traffic nodes using reinforcement learning (RL). This combined approach is called the Reinforcement Improved Golden Eagle Optimization (RIGEO) algorithm. Experimental results demonstrate that RIGEO achieves up to a 29% reduction in energy consumption, up to an 86% improvement in response time, and up to a 19% reduction in deadline violations compared to state-of-the-art algorithms.",
      "author": "Mohammad Sadegh Sirjani, Mohammad Ahmad, Amir Mousavi, Erfan Nourbakhsh, Khoa Nguyen",
      "source": "arXiv Hardware Architecture",
      "sourceType": "news",
      "pubDate": "Tue, 17 Feb 2026 00:00:00 -0500",
      "popularity": 0,
      "category": "inference-optimization"
    },
    {
      "title": "Scope: A Scalable Merged Pipeline Framework for Multi-Chip-Module NN Accelerators",
      "link": "https://arxiv.org/abs/2602.14393",
      "description": "arXiv:2602.14393v1 Announce Type: new \nAbstract: Neural network (NN) accelerators with multi-chip-module (MCM) architectures enable integration of massive computation capability; however, they face challenges of computing resource underutilization and off-chip communication overheads. Traditional parallelization schemes for NN inference on MCM architectures, such as intra-layer parallelism and inter-layer pipelining, show incompetency in breaking through both challenges, limiting the scalability of MCM architectures.\n  We observed that existing works typically deploy layers separately rather than considering them jointly. This underexploited dimension leads to compromises between system computation and communication, thus hindering optimal utilization, especially as hardware/software scale. To address this limitation, we propose Scope, a merged pipeline framework incorporating this overlooked multi-layer dimension, thereby achieving improved throughput and scalability by relaxing tradeoffs between computation, communication and memory costs. This new dimension, however, adds to the complexity of design space exploration (DSE). To tackle this, we develop a series of search algorithms that achieves exponential-to-linear complexity reduction, while identifying solutions that rank in the top 0.05% of performance. Experiments show that Scope achieves up to 1.73x throughput improvement while maintaining similar energy consumption for ResNet-152 inference compared to state-of-the-art approaches.",
      "content": "arXiv:2602.14393v1 Announce Type: new \nAbstract: Neural network (NN) accelerators with multi-chip-module (MCM) architectures enable integration of massive computation capability; however, they face challenges of computing resource underutilization and off-chip communication overheads. Traditional parallelization schemes for NN inference on MCM architectures, such as intra-layer parallelism and inter-layer pipelining, show incompetency in breaking through both challenges, limiting the scalability of MCM architectures.\n  We observed that existing works typically deploy layers separately rather than considering them jointly. This underexploited dimension leads to compromises between system computation and communication, thus hindering optimal utilization, especially as hardware/software scale. To address this limitation, we propose Scope, a merged pipeline framework incorporating this overlooked multi-layer dimension, thereby achieving improved throughput and scalability by relaxing tradeoffs between computation, communication and memory costs. This new dimension, however, adds to the complexity of design space exploration (DSE). To tackle this, we develop a series of search algorithms that achieves exponential-to-linear complexity reduction, while identifying solutions that rank in the top 0.05% of performance. Experiments show that Scope achieves up to 1.73x throughput improvement while maintaining similar energy consumption for ResNet-152 inference compared to state-of-the-art approaches.",
      "author": "Zongle Huang, Hongyang Jia, Kaiwei Zou, Yongpan Liu",
      "source": "arXiv Hardware Architecture",
      "sourceType": "news",
      "pubDate": "Tue, 17 Feb 2026 00:00:00 -0500",
      "popularity": 0,
      "category": "architecture"
    },
    {
      "title": "TrackCore-F: Deploying Transformer-Based Subatomic Particle Tracking on FPGAs",
      "link": "https://arxiv.org/abs/2509.26335",
      "description": "arXiv:2509.26335v2 Announce Type: replace-cross \nAbstract: The Transformer Machine Learning (ML) architecture has been gaining considerable momentum in recent years. In particular, computational High-Energy Physics tasks such as jet tagging and particle track reconstruction (tracking), have either achieved proper solutions, or reached considerable milestones using Transformers. On the other hand, the use of specialised hardware accelerators, especially FPGAs, is an effective method to achieve online, or pseudo-online latencies. The development and integration of Transformer-based ML to FPGAs is still ongoing and the support from current tools is very limited or non-existent. Additionally, FPGA resources present a significant constraint. Considering the model size alone, while smaller models can be deployed directly, larger models are to be partitioned in a meaningful and ideally, automated way. We aim to develop methodologies and tools for monolithic, or partitioned Transformer synthesis, specifically targeting inference. Our primary use-case involves two machine learning model designs for tracking, derived from the TrackFormers project. We elaborate our development approach, present preliminary results, and provide comparisons.",
      "content": "arXiv:2509.26335v2 Announce Type: replace-cross \nAbstract: The Transformer Machine Learning (ML) architecture has been gaining considerable momentum in recent years. In particular, computational High-Energy Physics tasks such as jet tagging and particle track reconstruction (tracking), have either achieved proper solutions, or reached considerable milestones using Transformers. On the other hand, the use of specialised hardware accelerators, especially FPGAs, is an effective method to achieve online, or pseudo-online latencies. The development and integration of Transformer-based ML to FPGAs is still ongoing and the support from current tools is very limited or non-existent. Additionally, FPGA resources present a significant constraint. Considering the model size alone, while smaller models can be deployed directly, larger models are to be partitioned in a meaningful and ideally, automated way. We aim to develop methodologies and tools for monolithic, or partitioned Transformer synthesis, specifically targeting inference. Our primary use-case involves two machine learning model designs for tracking, derived from the TrackFormers project. We elaborate our development approach, present preliminary results, and provide comparisons.",
      "author": "Arjan Blankestijn, Uraz Odyurt, Amirreza Yousefzadeh",
      "source": "arXiv Hardware Architecture",
      "sourceType": "news",
      "pubDate": "Tue, 17 Feb 2026 00:00:00 -0500",
      "popularity": 0,
      "category": "architecture"
    }
  ]
}